{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import html\n",
    "import re\n",
    "import contractions\n",
    "from copy import deepcopy\n",
    "from nltk import ngrams\n",
    "\n",
    "\n",
    "from fyp.crypto import Crypto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://link.springer.com/content/pdf/10.1007/11892755_87.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "crypto = Crypto()\n",
    "base = '/its/home/ep396/Documents/FYP/'\n",
    "name = \"dataset\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_db(name, base):\n",
    "    e = base + f\"encrypted_{name}.db\"\n",
    "    d = base + f\"decrypted_{name}.db\"\n",
    "    crypto.age_decrypt_file(e, d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unload_db(name, base):\n",
    "    e = base + f\"encrypted_{name}.db\"\n",
    "    d = base + f\"decrypted_{name}.db\"\n",
    "    crypto.age_encrypt_file(d, e)\n",
    "\n",
    "    os.remove(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_db(name, base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fyp.db_dataset import Tweet, ReferencedTweet, DataSplit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /its/home/ep396/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /its/home/ep396/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /its/home/ep396/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /its/home/ep396/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_phase(initial_set_s):\n",
    "    tweet_bigrams = {}\n",
    "    unique_bigrams = []\n",
    "    freqency = {}\n",
    "\n",
    "    for tweet_id, tweet_text in initial_set_s.items():\n",
    "        cleaned_text = clean_text(tweet_text)\n",
    "        eal = pre_processing_words(cleaned_text)\n",
    "        tweet_bigrams[tweet_id] = list(nltk.bigrams(eal))\n",
    "        for bigram in tweet_bigrams[tweet_id]:\n",
    "            if bigram not in unique_bigrams:\n",
    "                unique_bigrams.append(bigram)\n",
    "\n",
    "    for bigram in unique_bigrams:\n",
    "        for tweet_bigrams_collection in tweet_bigrams.values():\n",
    "            if bigram in tweet_bigrams_collection:\n",
    "                if bigram in freqency:\n",
    "                    freqency[bigram] += 1\n",
    "                else:\n",
    "                    freqency[bigram] = 1\n",
    "        \n",
    "    return freqency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "sw = nltk.corpus.stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_words(text):\n",
    "    tokenized = nltk.word_tokenize(text)\n",
    "    return tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    html.unescape(text)\n",
    "    removed_links = re.sub(r\"https?://\\S+\", '', text)\n",
    "    removed_mentions = re.sub(r\"(^|[^@\\w])@(\\w{1,15})\\b\", '', removed_links)\n",
    "    return removed_mentions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1581/1581 [00:00<00:00, 801256.00it/s]\n"
     ]
    }
   ],
   "source": [
    "init_user_author_id = Tweet.select().first().author_id\n",
    "init_user_tweets_query = Tweet.select(Tweet.tweet_id, Tweet.text).where(Tweet.author_id == init_user_author_id)\n",
    "init_user_corpus = {tweet.tweet_id:tweet.text for tweet in tqdm(init_user_tweets_query)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.775"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sum([len(text) for text in init_user_corpus.values()]) // len(init_user_corpus)) * 0.025\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_threshold = (sum([len(text) for text in init_user_corpus.values()]) // len(init_user_corpus)) * 0.025\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126.70549838244915\n"
     ]
    }
   ],
   "source": [
    "frequent_pairs = initial_phase(init_user_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "print(json.dumps(sorted(frequent_pairs.items(), key=lambda x:x[1], reverse=True), indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discovery_phase(original_grams, corpus):\n",
    "    k = 2\n",
    "    max_list = []\n",
    "    grams = deepcopy(original_grams)\n",
    "    while len(grams) > 0:\n",
    "        for g, frequency in grams.items():\n",
    "            if not_a_subsequence(g, max_list):\n",
    "                if frequency > 1:\n",
    "                    max = expand(g)\n",
    "                    max_list.append(max)\n",
    "                    if max == g:\n",
    "                        grams.pop(g, None)\n",
    "                else:\n",
    "                    grams.pop(g, None)\n",
    "        \n",
    "        grams = form_grams_plus_one(grams, corpus)\n",
    "        k += 1\n",
    "    \n",
    "    return max_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bigrams = {(\"hello\", \"world\") : 10, (\"world\", \"hi\"): 12, (\"hi\", \"hello\"): 1}\n",
    "test_corpus = {1: \"hello world hi hello world hello world hi\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_grams_plus_one(grams, corpus):\n",
    "    n_grams = []\n",
    "    n_grams_frequencies = {}\n",
    "\n",
    "    for gram_one in grams.keys():\n",
    "        for gram_two in grams.keys():\n",
    "            if gram_one[-1] == gram_two[0]:\n",
    "                n_grams.append(combine_grams(gram_one, gram_two))\n",
    "            if gram_two[-1] == gram_one[0]:\n",
    "                n_grams.append(combine_grams(gram_two, gram_one))\n",
    "\n",
    "    for gram in n_grams:\n",
    "        for tweet_text in corpus.values():\n",
    "            tokenized = nltk.word_tokenize(tweet_text)\n",
    "            tweet_text_ngrams = ngrams(tokenized, len(gram))\n",
    "            if gram in tweet_text_ngrams:\n",
    "                if gram not in n_grams_frequencies:\n",
    "                    n_grams_frequencies[gram] = 1\n",
    "                else:\n",
    "                    n_grams_frequencies[gram] += 1\n",
    "\n",
    "    return n_grams_frequencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_grams(left_gram, right_gram):\n",
    "    left_list = list(left_gram)\n",
    "    right_list = list(right_gram)\n",
    "    right_list.pop(0)\n",
    "    return tuple(left_list + right_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def not_a_subsequence(g, max_list):\n",
    "    grams = [m for m in max_list]\n",
    "    for gram in grams:\n",
    "        if len(g) <= len(gram):\n",
    "            if not is_a_in_x(g, gram):\n",
    "                return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_a_in_x(A, X):\n",
    "    for i in range(len(X) - len(A) + 1):\n",
    "        if A == X[i:i+len(A)]: return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand(p):\n",
    "    l = len(p)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unload_db(name, base)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6ade73bf49f1256608149ff920ccd937fcccdc8efd4975ff38aa98fc4d821ac5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('fyp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
