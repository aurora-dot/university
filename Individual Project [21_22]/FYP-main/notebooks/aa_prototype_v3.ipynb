{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import html\n",
    "import re\n",
    "import contractions\n",
    "import string\n",
    "import emoji\n",
    "from collections import Counter\n",
    "\n",
    "from fyp.crypto import Crypto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://aclanthology.org/W13-1107.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /its/home/ep396/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /its/home/ep396/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /its/home/ep396/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /its/home/ep396/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /its/home/ep396/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     /its/home/ep396/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('tagsets')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://c.r74n.com/faces\n",
    "happy_emojis = [':)',':-)',':]',':-]',':3',':-3',':>',':->','8)','8-)',':}',':-}',':o)',':o]',':o3',':o>',':o}',':c)',':c]',':c3',':c>',':c}',':^)',':^]',':^3',':^>',':^}','=^)','=^]','=^3','=^>','=^}','=)','=-)','=]','=-]','=3','=-3','=>','=->','=}','=-}','=o)','=o]','=o3','=o>','=o}','=c)','=c]','=c3','=c>','=c}','(:','(-:','[:','[-:','Ɛ:','Ɛ-:','<:','<-:','(8','(-8','{:','{-:','(o:','[o:','Ɛo:','<o:','{o:','(ɔ:','[ɔ:','Ɛɔ:','<ɔ:','{ɔ:','(^:','[^:','Ɛ^:','<^:','{^:','(^=','[^=','Ɛ^=','<^=','{^=','(=','(-=','[=','[-=','Ɛ=','Ɛ-=','<=','<-=','{=','{-=','(o=','[o=','Ɛo=','<o=','{o=','(ɔ=','[ɔ=','Ɛɔ=','<ɔ=','{ɔ=',':))',':)))',':))))',':)))))',':))))))',':)))))))',':))))))))',':)))))))))','¦)','¦¬)','¦-)','¦^)','¦o)','¦c)','(¦','(-¦','(^¦','(o¦','(c¦','〓)','〓¬)','〓-)','〓^)','〓o)','〓c)','(〓','(-〓','(^〓','(o〓','(c〓','C:','C-:','C^:','c:','c-:','c^:',':Ↄ',':-Ↄ',':^Ↄ',':ɔ',':-ɔ',':^ɔ',':D',':-D','8D','8-D','=D','=-D',':^D','8^D','=^D','BD','B-D','B^D',';D',';-D',';^D',':ↁ','〓D','¦D','^^','^ ^','^-^','^_^','^.^','^,^','^u^','^w^','^ㅂ^','^o^','^U^','^O^','^0^','^_________^','^오^','n.n','n_n','•ᴗ•','.^◡^.','´͈ ᵕ `͈','¢‿¢','°ﺑ°','ó‿ó','ôヮô','ʘ‿ʘ','◕ω◕','◕‿◕','◕◡◕','◕ ◡ ◕','◘‿◘','◙‿◙','☻_☻','☻‿☻','Ꙩ⌵Ꙩ','.⌵.','-⌵-','\\'⌵\\'','>⌵<','(^_^)','(^ ^)','(^O^)','(^o^)','(^^)','(≧∇≦)','(◕ヮ◕)','(^.^)','(^·^)','(*^0^*)','!(^^)!','(●＾o＾●)','(＾ｖ＾)','(＾ｕ＾)','(＾◇＾)','( ^)o(^ )','(^○^)','＼(^o^)／','\\(^o^)/','(*^▽^*)','(✿◠‿◠)','ヽ(´ー｀)ﾉ','（´∀｀）','( ﾟヮﾟ)','d(*⌒▽⌒*)b','(◕‿◕✿)','(◡‿◡✿)','(≧ ᗜ ≦)','(⌃·̫⌃)','( ⁼̴̤̆◡̶͂⁼̴̤̆ )','Σ ◕ ◡ ◕','٩(•̮̮̃•̃)۶','٩(●̮̮̃●̃)۶','٩(｡͡•‿•｡)۶','۹⌤_⌤۹','ლ(╹◡╹ლ)','uwu','UwU','UuU','UvU','UvvU','◕◡◕','◕v◕','◕u◕','◕w◕','Ü Ü','ü ü','v̈','V̈','Ѷ','ẅ','Ẅ','(Ü)','⏝̈','⏝̎','◟̆◞̆','◟̊◞̊','◡̈','ᵔ.ᵔ','ᵔ◡ᵔ','\\'◡\\'','ツ','シ','ッ','ヅ','ツ゚','ϡ','ジ','ｼ','ﾂ','㋡','㋛','☺','☻','〠','(ツ)','⍢','⍢⃝','ت','ﭢ','ت']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "crypto = Crypto()\n",
    "base = '/its/home/ep396/Documents/FYP/'\n",
    "name = \"dataset\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_db(name, base):\n",
    "    e = base + f\"encrypted_{name}.db\"\n",
    "    d = base + f\"decrypted_{name}.db\"\n",
    "    crypto.age_decrypt_file(e, d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unload_db(name, base):\n",
    "    e = base + f\"encrypted_{name}.db\"\n",
    "    d = base + f\"decrypted_{name}.db\"\n",
    "    crypto.age_encrypt_file(d, e)\n",
    "\n",
    "    os.remove(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_db(name, base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fyp.db_dataset import Tweet, ReferencedTweet, DataSplit, ExtractedFeatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1106/1106 [00:00<00:00, 800638.63it/s]\n"
     ]
    }
   ],
   "source": [
    "init_user_author_id = Tweet.select().first().author_id\n",
    "init_user_tweets_query = Tweet.select(Tweet.tweet_id, Tweet.text).join(DataSplit).where(Tweet.author_id == init_user_author_id, DataSplit.split_type == 0)\n",
    "init_user_corpus = {tweet.tweet_id:tweet.text for tweet in tqdm(init_user_tweets_query)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    html.unescape(text)\n",
    "    removed_links = re.sub(r\"https?://\\S+\", '', text)\n",
    "    removed_mentions = re.sub(r\"(^|[^@\\w])@(\\w{1,15})\\b\", '', removed_links)\n",
    "    return removed_mentions.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_num_features_from_document(text):\n",
    "    num_of_characters = len(text)\n",
    "    num_of_sentences = get_sentence_count(text)\n",
    "    num_of_tokens = get_token_count(text)\n",
    "    num_of_no_vowels = get_no_vowels_count(text)\n",
    "    num_of_alphabet_count = get_alphabet_count(text)\n",
    "    num_of_punctuation_count = get_punctuation_count(text)\n",
    "    num_of_two_three_continuous_punctuation_count = get_two_three_continuous_punctuation(text)\n",
    "    num_of_contraction_count = get_contraction_count(text)\n",
    "    num_of_parenthesis_count = get_parenthesis_count(text)\n",
    "    num_of_all_caps_letter_word_count = get_all_caps_letter_word_count(text)\n",
    "    num_of_emoticons_count = get_emoticons_count(text)\n",
    "    num_of_happy_emoticons_count = get_happy_emoticons_count(text)\n",
    "    num_of_sentence_without_capital_at_beginning = get_sentence_without_capital_at_beginning(text)\n",
    "    num_of_quotation = get_quotation(text)\n",
    "\n",
    "    return [\n",
    "        num_of_characters,\n",
    "        num_of_sentences,\n",
    "        num_of_tokens,\n",
    "        num_of_no_vowels,\n",
    "        num_of_alphabet_count,\n",
    "        num_of_punctuation_count,\n",
    "        num_of_two_three_continuous_punctuation_count,\n",
    "        num_of_contraction_count,\n",
    "        num_of_parenthesis_count,\n",
    "        num_of_all_caps_letter_word_count,\n",
    "        num_of_emoticons_count,\n",
    "        num_of_happy_emoticons_count,\n",
    "        num_of_sentence_without_capital_at_beginning,\n",
    "        num_of_quotation\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_count(text):\n",
    "    return len(nltk.sent_tokenize(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_count(text):\n",
    "    return len(nltk.word_tokenize(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_no_vowels_count(text):\n",
    "    count = 0\n",
    "    vowels = ['a', 'e', 'i', 'o', 'u']\n",
    "    tokenized = nltk.word_tokenize(text.lower())\n",
    "\n",
    "    for word in tokenized:\n",
    "        if not any(v in word for v in vowels):\n",
    "            count += 1\n",
    "    return count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alphabet_count(text):\n",
    "    return sum([1 for letter in text if letter.isalpha()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_punctuation_count(text):\n",
    "    return sum([1 for letter in text if letter in string.punctuation])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_two_three_continuous_punctuation(text):\n",
    "    return sum([1 for group in re.findall(r\"([^\\s\\w]+)\", text) if len(group) > 1 and len(group) < 4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contraction_count(text):\n",
    "    return sum([1 for word in re.findall(r\"[\\w']+\", text) if word != contractions.fix(word)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parenthesis_count(text):\n",
    "    return len(re.findall(r\"(,.*?,)|(\\(.*?\\))|(-.*?-)\", text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_caps_letter_word_count(text):\n",
    "    return sum([1 for word in nltk.word_tokenize(text) if word.upper() == word and word not in string.punctuation])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emoticons_count(text):\n",
    "    tokenized = nltk.casual_tokenize(text)\n",
    "    count = 0\n",
    "\n",
    "    for token in tokenized:\n",
    "        if emoji.emojize(token) in emoji.UNICODE_EMOJI['en']:\n",
    "            count += 1\n",
    "        elif nltk.tokenize.casual.EMOTICON_RE.search(token):\n",
    "            count += 1\n",
    "\n",
    "    return count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_happy_emoticons_count(text):\n",
    "    tokenized = nltk.casual_tokenize(text)\n",
    "    count = 0\n",
    "\n",
    "    for token in tokenized:\n",
    "        emojized = emoji.emojize(token)\n",
    "        if emojized in emoji.UNICODE_EMOJI['en'] and emojized in [\"😺\", \"🤗\", \"😀\", \"🙂\", \"😃\", \"☺️\", \"😊\", \"😸\", \"😹\", \"😂\"]:\n",
    "            count += 1\n",
    "        elif token in happy_emojis:\n",
    "            count += 1\n",
    "\n",
    "    return count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_without_capital_at_beginning(text):\n",
    "    return sum([1 for sentence in nltk.sent_tokenize(text) if (sentence[0].lower() == sentence[0]) or (sentence[0].isnumeric())])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "<class 'str'>\n",
      "1\n",
      "1\n",
      "---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sentence_without_capital_at_beginning(\"1 kgvg kghbkg.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quotation(text):\n",
    "    return len(re.findall(r'\"[^\"]*\"', text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = clean_text(test_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_sentences = get_sentence_count(text)\n",
    "num_of_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_tokens = get_token_count(text)\n",
    "num_of_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_no_vowels = get_no_vowels_count(text)\n",
    "num_of_no_vowels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_alphabet_count = get_alphabet_count(text)\n",
    "num_of_alphabet_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_punctuation_count = get_punctuation_count(text)\n",
    "num_of_punctuation_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_two_three_continuous_punctuation_count = get_two_three_continuous_punctuation(\"hfvhf eh cher !!!\")\n",
    "num_of_two_three_continuous_punctuation_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_contraction_count = get_contraction_count(\"should've, hadn't-hadn't, hi\")\n",
    "num_of_contraction_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_parenthesis_count = get_parenthesis_count(\"hello, he said, while walking (slowly) why - did i - do this.\")\n",
    "num_of_parenthesis_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_all_caps_letter_word_count = get_all_caps_letter_word_count(text)\n",
    "num_of_all_caps_letter_word_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_emoticons_count = get_emoticons_count(\"hello world 🦖 :hfdjhgvgh: :)\")\n",
    "num_of_emoticons_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_happy_emoticons_count = get_happy_emoticons_count(\"hello world 🦖 :hfdjhgvgh: :) 😺\")\n",
    "num_of_happy_emoticons_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_sentence_without_capital_at_beginning = get_sentence_without_capital_at_beginning(text)\n",
    "num_of_sentence_without_capital_at_beginning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_of_quotation = get_quotation('hello \"don\\'t\" do')\n",
    "num_of_quotation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[246, 3, 51, 5, 194, 4, 0, 51, 0, 1, 0, 0, 1, 0]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_num_features_from_document(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_freq_features_from_document(text):\n",
    "    freq_pos_tags = get_freq_pos_tags(text)\n",
    "    freq_of_letters = get_freq_of_letters(text)\n",
    "    freq_function_words = get_freq_function_words(text)\n",
    "    freq_lower_i = get_freq_lower_i(text)\n",
    "    freq_stop_without_white_space = get_freq_stop_without_white_space(text)\n",
    "    freq_question = get_freq_question(text)\n",
    "    freq_sentence_with_small_letter = get_freq_sentence_with_small_letter(text)\n",
    "    freq_alpha_digit_uppercase_whitespace_tab = get_freq_alpha_digit_uppercase_whitespace_tab(text)\n",
    "    freq_a_and_an_error = get_freq_a_and_an_error(text)\n",
    "    freq_he_she_they = get_freq_he_she_they(text)\n",
    "\n",
    "    return [\n",
    "        freq_pos_tags,\n",
    "        freq_of_letters,\n",
    "        freq_function_words,\n",
    "        freq_lower_i,\n",
    "        freq_stop_without_white_space,\n",
    "        freq_question,\n",
    "        freq_sentence_with_small_letter,\n",
    "        freq_alpha_digit_uppercase_whitespace_tab,\n",
    "        freq_a_and_an_error,\n",
    "        freq_he_she_they\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freq_pos_tags(text):\n",
    "    tags_frequencies = {\"CC\": 0,\"CD\": 0,\"DT\": 0,\"EX\": 0,\"FW\": 0,\"IN\": 0,\"JJ\": 0,\"JJR\": 0,\"JJS\": 0,\"LS\": 0,\"MD\": 0,\"NN\": 0,\"NNP\": 0,\"NNPS\": 0,\"NNS\": 0,\"PDT\": 0,\"POS\": 0,\"PRP\": 0,\"PRP$\": 0,\"RB\": 0,\"RBR\": 0,\"RBS\": 0,\"RP\": 0,\"SYM\": 0,\"TO\": 0,\"UH\": 0,\"VB\": 0,\"VBD\": 0,\"VBG\": 0,\"VBN\": 0,\"VBP\": 0,\"VBZ\": 0,\"WDT\": 0,\"WP\": 0,\"WP$\": 0,\"WRB\": 0}\n",
    "    lower_case = text.lower()\n",
    "    tokens = nltk.word_tokenize(lower_case)\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "\n",
    "    for tag_type, count in Counter(tag for word, tag in tags).items():\n",
    "        if tag_type in tags_frequencies: tags_frequencies[tag_type] = count\n",
    "    \n",
    "    return tags_frequencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freq_of_letters(text):\n",
    "    frequencies = {char:0 for char in string.ascii_lowercase}\n",
    "    total = 0\n",
    "\n",
    "    for char in text.lower():\n",
    "        if char in frequencies:\n",
    "            frequencies[char] += 1\n",
    "            total += 1\n",
    "\n",
    "    for char, freq in frequencies.items():\n",
    "        frequencies[char] = freq / total\n",
    "\n",
    "    return frequencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freq_function_words(text):\n",
    "    # 10.1002/asi.20316\n",
    "    functional_words_freq = {\"a\": 0,\"between\": 0,\"in\": 0,\"nor\": 0,\"some\": 0,\"upon\": 0,\"about\": 0,\"both\": 0,\"including\": 0,\"nothing\": 0,\"somebody\": 0,\"us\": 0,\"above\": 0,\"but\": 0,\"inside\": 0,\"of\": 0,\"someone\": 0,\"used\": 0,\"after\": 0,\"by\": 0,\"into\": 0,\"off\": 0,\"something\": 0,\"via\": 0,\"all\": 0,\"can\": 0,\"is\": 0,\"on\": 0,\"such\": 0,\"we\": 0,\"although\": 0,\"cos\": 0,\"it\": 0,\"once\": 0,\"than\": 0,\"what\": 0,\"am\": 0,\"do\": 0,\"its\": 0,\"one\": 0,\"that\": 0,\"whatever\": 0,\"among\": 0,\"down\": 0,\"latter\": 0,\"onto\": 0,\"the\": 0,\"when\": 0,\"an\": 0,\"each\": 0,\"less\": 0,\"opposite\": 0,\"their\": 0,\"where\": 0,\"and\": 0,\"either\": 0,\"like\": 0,\"or\": 0,\"them\": 0,\"whether\": 0,\"another\": 0,\"enough\": 0,\"little\": 0,\"our\": 0,\"these\": 0,\"which\": 0,\"any\": 0,\"every\": 0,\"lots\": 0,\"outside\": 0,\"they\": 0,\"while\": 0,\"anybody\": 0,\"everybody\": 0,\"many\": 0,\"over\": 0,\"this\": 0,\"who\": 0,\"anyone\": 0,\"everyone\": 0,\"me\": 0,\"own\": 0,\"those\": 0,\"whoever\": 0,\"anything\": 0,\"everything\": 0,\"more\": 0,\"past\": 0,\"though\": 0,\"whom\": 0,\"are\": 0,\"few\": 0,\"most\": 0,\"per\": 0,\"through\": 0,\"whose\": 0,\"around\": 0,\"following\": 0,\"much\": 0,\"plenty\": 0,\"till\": 0,\"will\": 0,\"as\": 0,\"for\": 0,\"must\": 0,\"plus\": 0,\"to\": 0,\"with\": 0,\"at\": 0,\"from\": 0,\"my\": 0,\"regarding\": 0,\"toward\": 0,\"within\": 0,\"be\": 0,\"have\": 0,\"near\": 0,\"same\": 0,\"towards\": 0,\"without\": 0,\"because\": 0,\"he\": 0,\"need\": 0,\"several\": 0,\"under\": 0,\"worth\": 0,\"before\": 0,\"her\": 0,\"neither\": 0,\"she\": 0,\"unless\": 0,\"would\": 0,\"behind\": 0,\"him\": 0,\"no\": 0,\"should\": 0,\"unlike\": 0,\"yes\": 0,\"below\": 0,\"i\": 0,\"nobody\": 0,\"since\": 0,\"until\": 0,\"you\": 0,\"beside\": 0,\"if\": 0,\"none\": 0,\"so\": 0,\"up\": 0,\"your\": 0}\n",
    "    for word in nltk.casual_tokenize(text.lower()):\n",
    "        if word in functional_words_freq:\n",
    "            functional_words_freq[word] += 1\n",
    "    return functional_words_freq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freq_lower_i(text):\n",
    "    return sum([1 for word in nltk.casual_tokenize(text) if word == \"i\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freq_stop_without_white_space(text):\n",
    "    return len(re.findall(r\"\\.\\w\", text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freq_question(text):\n",
    "    return text.count('?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freq_sentence_with_small_letter(text):\n",
    "    return sum([1 for sentence in nltk.sent_tokenize(text) if sentence[0].lower() == sentence[0] and sentence[0].isalpha()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freq_alpha_digit_uppercase_whitespace_tab(text):\n",
    "    freq = {\"alpha\": 0, \"digit\": 0, \"uppercase\": 0, \"whitespace\": text.count(\" \"), \"tab\": text.count(\"\\t\")}\n",
    "    for char in text:\n",
    "        if char.isalpha():\n",
    "            freq[\"alpha\"] += 1\n",
    "            if char.upper() == char:\n",
    "                freq[\"uppercase\"] += 1\n",
    "        elif char.isnumeric():\n",
    "            freq[\"digit\"] += 1\n",
    "    \n",
    "    return freq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freq_a_and_an_error(text):\n",
    "    freq = {\"a\": 0, \"an\": 0}\n",
    "    bigrams = nltk.bigrams(nltk.casual_tokenize(text.lower()))\n",
    "    vowels = ['a', 'e', 'i', 'o', 'u']\n",
    "\n",
    "    for bigram in bigrams:\n",
    "        if bigram[0] in freq and bigram[1].isalpha():\n",
    "            if bigram[1][0] in vowels and bigram[0] == \"a\":\n",
    "                freq[\"a\"] += 1\n",
    "            elif bigram[1][0] not in vowels and bigram[0] == \"an\":\n",
    "                freq[\"an\"] += 1\n",
    "\n",
    "    return freq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freq_he_she_they(text):\n",
    "    frequency = {\"he\": 0, \"she\": 0, \"they\": 0}\n",
    "    for token in nltk.casual_tokenize(text.lower()):\n",
    "        if token in frequency:\n",
    "            frequency[token] += 1\n",
    "    \n",
    "    return frequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CC': 1,\n",
       " 'CD': 1,\n",
       " 'DT': 8,\n",
       " 'EX': 1,\n",
       " 'FW': 0,\n",
       " 'IN': 9,\n",
       " 'JJ': 6,\n",
       " 'JJR': 0,\n",
       " 'JJS': 0,\n",
       " 'LS': 0,\n",
       " 'MD': 1,\n",
       " 'NN': 12,\n",
       " 'NNP': 0,\n",
       " 'NNPS': 0,\n",
       " 'NNS': 2,\n",
       " 'PDT': 0,\n",
       " 'POS': 0,\n",
       " 'PRP': 0,\n",
       " 'PRP$': 0,\n",
       " 'RB': 0,\n",
       " 'RBR': 0,\n",
       " 'RBS': 0,\n",
       " 'RP': 0,\n",
       " 'SYM': 0,\n",
       " 'TO': 2,\n",
       " 'UH': 0,\n",
       " 'VB': 3,\n",
       " 'VBD': 0,\n",
       " 'VBG': 0,\n",
       " 'VBN': 0,\n",
       " 'VBP': 1,\n",
       " 'VBZ': 2,\n",
       " 'WDT': 0,\n",
       " 'WP': 0,\n",
       " 'WP$': 0,\n",
       " 'WRB': 0}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_pos_tags = get_freq_pos_tags(text)\n",
    "freq_pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0.08247422680412371,\n",
       " 'b': 0.010309278350515464,\n",
       " 'c': 0.020618556701030927,\n",
       " 'd': 0.030927835051546393,\n",
       " 'e': 0.11855670103092783,\n",
       " 'f': 0.041237113402061855,\n",
       " 'g': 0.03608247422680412,\n",
       " 'h': 0.041237113402061855,\n",
       " 'i': 0.04639175257731959,\n",
       " 'j': 0.0,\n",
       " 'k': 0.0,\n",
       " 'l': 0.06701030927835051,\n",
       " 'm': 0.005154639175257732,\n",
       " 'n': 0.08762886597938144,\n",
       " 'o': 0.10309278350515463,\n",
       " 'p': 0.020618556701030927,\n",
       " 'q': 0.0,\n",
       " 'r': 0.07216494845360824,\n",
       " 's': 0.05670103092783505,\n",
       " 't': 0.08762886597938144,\n",
       " 'u': 0.03608247422680412,\n",
       " 'v': 0.015463917525773196,\n",
       " 'w': 0.005154639175257732,\n",
       " 'x': 0.005154639175257732,\n",
       " 'y': 0.010309278350515464,\n",
       " 'z': 0.0}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_of_letters = get_freq_of_letters(text)\n",
    "freq_of_letters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0,\n",
       " 'between': 0,\n",
       " 'in': 1,\n",
       " 'nor': 0,\n",
       " 'some': 0,\n",
       " 'upon': 0,\n",
       " 'about': 0,\n",
       " 'both': 0,\n",
       " 'including': 0,\n",
       " 'nothing': 0,\n",
       " 'somebody': 0,\n",
       " 'us': 0,\n",
       " 'above': 0,\n",
       " 'but': 0,\n",
       " 'inside': 0,\n",
       " 'of': 3,\n",
       " 'someone': 0,\n",
       " 'used': 0,\n",
       " 'after': 0,\n",
       " 'by': 0,\n",
       " 'into': 0,\n",
       " 'off': 0,\n",
       " 'something': 0,\n",
       " 'via': 0,\n",
       " 'all': 1,\n",
       " 'can': 0,\n",
       " 'is': 2,\n",
       " 'on': 1,\n",
       " 'such': 0,\n",
       " 'we': 0,\n",
       " 'although': 0,\n",
       " 'cos': 0,\n",
       " 'it': 0,\n",
       " 'once': 0,\n",
       " 'than': 0,\n",
       " 'what': 0,\n",
       " 'am': 0,\n",
       " 'do': 0,\n",
       " 'its': 0,\n",
       " 'one': 0,\n",
       " 'that': 0,\n",
       " 'whatever': 0,\n",
       " 'among': 0,\n",
       " 'down': 0,\n",
       " 'latter': 0,\n",
       " 'onto': 0,\n",
       " 'the': 5,\n",
       " 'when': 0,\n",
       " 'an': 0,\n",
       " 'each': 0,\n",
       " 'less': 0,\n",
       " 'opposite': 0,\n",
       " 'their': 0,\n",
       " 'where': 0,\n",
       " 'and': 1,\n",
       " 'either': 0,\n",
       " 'like': 0,\n",
       " 'or': 0,\n",
       " 'them': 0,\n",
       " 'whether': 0,\n",
       " 'another': 0,\n",
       " 'enough': 0,\n",
       " 'little': 0,\n",
       " 'our': 0,\n",
       " 'these': 0,\n",
       " 'which': 0,\n",
       " 'any': 0,\n",
       " 'every': 0,\n",
       " 'lots': 0,\n",
       " 'outside': 0,\n",
       " 'they': 0,\n",
       " 'while': 0,\n",
       " 'anybody': 0,\n",
       " 'everybody': 0,\n",
       " 'many': 0,\n",
       " 'over': 0,\n",
       " 'this': 0,\n",
       " 'who': 0,\n",
       " 'anyone': 0,\n",
       " 'everyone': 0,\n",
       " 'me': 0,\n",
       " 'own': 0,\n",
       " 'those': 0,\n",
       " 'whoever': 0,\n",
       " 'anything': 0,\n",
       " 'everything': 0,\n",
       " 'more': 0,\n",
       " 'past': 0,\n",
       " 'though': 0,\n",
       " 'whom': 0,\n",
       " 'are': 0,\n",
       " 'few': 0,\n",
       " 'most': 0,\n",
       " 'per': 0,\n",
       " 'through': 0,\n",
       " 'whose': 0,\n",
       " 'around': 0,\n",
       " 'following': 0,\n",
       " 'much': 0,\n",
       " 'plenty': 0,\n",
       " 'till': 0,\n",
       " 'will': 1,\n",
       " 'as': 1,\n",
       " 'for': 2,\n",
       " 'must': 0,\n",
       " 'plus': 0,\n",
       " 'to': 2,\n",
       " 'with': 0,\n",
       " 'at': 1,\n",
       " 'from': 0,\n",
       " 'my': 0,\n",
       " 'regarding': 0,\n",
       " 'toward': 0,\n",
       " 'within': 0,\n",
       " 'be': 1,\n",
       " 'have': 1,\n",
       " 'near': 0,\n",
       " 'same': 0,\n",
       " 'towards': 0,\n",
       " 'without': 0,\n",
       " 'because': 0,\n",
       " 'he': 0,\n",
       " 'need': 0,\n",
       " 'several': 0,\n",
       " 'under': 0,\n",
       " 'worth': 0,\n",
       " 'before': 0,\n",
       " 'her': 0,\n",
       " 'neither': 0,\n",
       " 'she': 0,\n",
       " 'unless': 0,\n",
       " 'would': 0,\n",
       " 'behind': 0,\n",
       " 'him': 0,\n",
       " 'no': 2,\n",
       " 'should': 0,\n",
       " 'unlike': 0,\n",
       " 'yes': 0,\n",
       " 'below': 0,\n",
       " 'i': 0,\n",
       " 'nobody': 0,\n",
       " 'since': 0,\n",
       " 'until': 0,\n",
       " 'you': 0,\n",
       " 'beside': 0,\n",
       " 'if': 0,\n",
       " 'none': 0,\n",
       " 'so': 0,\n",
       " 'up': 0,\n",
       " 'your': 0}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_function_words = get_freq_function_words(text)\n",
    "freq_function_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_lower_i = get_freq_lower_i(\"hello i I I i i hi\")\n",
    "freq_lower_i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_stop_without_white_space = get_freq_stop_without_white_space(\"hello, this is a normal sentence. this is a sentence which will capture after.hello world.10 here too., but not here\")\n",
    "freq_stop_without_white_space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_question = get_freq_question(\"hello world???\")\n",
    "freq_question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_sentence_with_small_letter = get_freq_sentence_with_small_letter(text)\n",
    "freq_sentence_with_small_letter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 197, 'digit': 1, 'uppercase': 6, 'whitespace': 46, 'tab': 0}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_alpha_digit_uppercase_whitespace_tab = get_freq_alpha_digit_uppercase_whitespace_tab(text)\n",
    "freq_alpha_digit_uppercase_whitespace_tab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_a_and_an_error = get_freq_a_and_an_error(\"an i an h a i a h\")\n",
    "freq_a_and_an_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'he': 1, 'she': 1, 'they': 1}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_he_she_they = get_freq_he_she_they(\"he she they\")\n",
    "freq_he_she_they\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'CC': 1,\n",
       "  'CD': 1,\n",
       "  'DT': 8,\n",
       "  'EX': 1,\n",
       "  'FW': 0,\n",
       "  'IN': 9,\n",
       "  'JJ': 6,\n",
       "  'JJR': 0,\n",
       "  'JJS': 0,\n",
       "  'LS': 0,\n",
       "  'MD': 1,\n",
       "  'NN': 12,\n",
       "  'NNP': 0,\n",
       "  'NNPS': 0,\n",
       "  'NNS': 2,\n",
       "  'PDT': 0,\n",
       "  'POS': 0,\n",
       "  'PRP': 0,\n",
       "  'PRP$': 0,\n",
       "  'RB': 0,\n",
       "  'RBR': 0,\n",
       "  'RBS': 0,\n",
       "  'RP': 0,\n",
       "  'SYM': 0,\n",
       "  'TO': 2,\n",
       "  'UH': 0,\n",
       "  'VB': 3,\n",
       "  'VBD': 0,\n",
       "  'VBG': 0,\n",
       "  'VBN': 0,\n",
       "  'VBP': 1,\n",
       "  'VBZ': 2,\n",
       "  'WDT': 0,\n",
       "  'WP': 0,\n",
       "  'WP$': 0,\n",
       "  'WRB': 0},\n",
       " {'a': 0.08121827411167512,\n",
       "  'b': 0.015228426395939087,\n",
       "  'c': 0.02030456852791878,\n",
       "  'd': 0.030456852791878174,\n",
       "  'e': 0.116751269035533,\n",
       "  'f': 0.04060913705583756,\n",
       "  'g': 0.03553299492385787,\n",
       "  'h': 0.04060913705583756,\n",
       "  'i': 0.050761421319796954,\n",
       "  'j': 0.0,\n",
       "  'k': 0.0,\n",
       "  'l': 0.06598984771573604,\n",
       "  'm': 0.005076142131979695,\n",
       "  'n': 0.08629441624365482,\n",
       "  'o': 0.1065989847715736,\n",
       "  'p': 0.02030456852791878,\n",
       "  'q': 0.0,\n",
       "  'r': 0.07106598984771574,\n",
       "  's': 0.05583756345177665,\n",
       "  't': 0.08629441624365482,\n",
       "  'u': 0.03553299492385787,\n",
       "  'v': 0.015228426395939087,\n",
       "  'w': 0.005076142131979695,\n",
       "  'x': 0.005076142131979695,\n",
       "  'y': 0.01015228426395939,\n",
       "  'z': 0.0},\n",
       " {'a': 0,\n",
       "  'between': 0,\n",
       "  'in': 1,\n",
       "  'nor': 0,\n",
       "  'some': 0,\n",
       "  'upon': 0,\n",
       "  'about': 0,\n",
       "  'both': 0,\n",
       "  'including': 0,\n",
       "  'nothing': 0,\n",
       "  'somebody': 0,\n",
       "  'us': 0,\n",
       "  'above': 0,\n",
       "  'but': 0,\n",
       "  'inside': 0,\n",
       "  'of': 3,\n",
       "  'someone': 0,\n",
       "  'used': 0,\n",
       "  'after': 0,\n",
       "  'by': 0,\n",
       "  'into': 0,\n",
       "  'off': 0,\n",
       "  'something': 0,\n",
       "  'via': 0,\n",
       "  'all': 1,\n",
       "  'can': 0,\n",
       "  'is': 2,\n",
       "  'on': 1,\n",
       "  'such': 0,\n",
       "  'we': 0,\n",
       "  'although': 0,\n",
       "  'cos': 0,\n",
       "  'it': 0,\n",
       "  'once': 0,\n",
       "  'than': 0,\n",
       "  'what': 0,\n",
       "  'am': 0,\n",
       "  'do': 0,\n",
       "  'its': 0,\n",
       "  'one': 0,\n",
       "  'that': 0,\n",
       "  'whatever': 0,\n",
       "  'among': 0,\n",
       "  'down': 0,\n",
       "  'latter': 0,\n",
       "  'onto': 0,\n",
       "  'the': 5,\n",
       "  'when': 0,\n",
       "  'an': 0,\n",
       "  'each': 0,\n",
       "  'less': 0,\n",
       "  'opposite': 0,\n",
       "  'their': 0,\n",
       "  'where': 0,\n",
       "  'and': 1,\n",
       "  'either': 0,\n",
       "  'like': 0,\n",
       "  'or': 0,\n",
       "  'them': 0,\n",
       "  'whether': 0,\n",
       "  'another': 0,\n",
       "  'enough': 0,\n",
       "  'little': 0,\n",
       "  'our': 0,\n",
       "  'these': 0,\n",
       "  'which': 0,\n",
       "  'any': 0,\n",
       "  'every': 0,\n",
       "  'lots': 0,\n",
       "  'outside': 0,\n",
       "  'they': 0,\n",
       "  'while': 0,\n",
       "  'anybody': 0,\n",
       "  'everybody': 0,\n",
       "  'many': 0,\n",
       "  'over': 0,\n",
       "  'this': 0,\n",
       "  'who': 0,\n",
       "  'anyone': 0,\n",
       "  'everyone': 0,\n",
       "  'me': 0,\n",
       "  'own': 0,\n",
       "  'those': 0,\n",
       "  'whoever': 0,\n",
       "  'anything': 0,\n",
       "  'everything': 0,\n",
       "  'more': 0,\n",
       "  'past': 0,\n",
       "  'though': 0,\n",
       "  'whom': 0,\n",
       "  'are': 0,\n",
       "  'few': 0,\n",
       "  'most': 0,\n",
       "  'per': 0,\n",
       "  'through': 0,\n",
       "  'whose': 0,\n",
       "  'around': 0,\n",
       "  'following': 0,\n",
       "  'much': 0,\n",
       "  'plenty': 0,\n",
       "  'till': 0,\n",
       "  'will': 1,\n",
       "  'as': 1,\n",
       "  'for': 2,\n",
       "  'must': 0,\n",
       "  'plus': 0,\n",
       "  'to': 2,\n",
       "  'with': 0,\n",
       "  'at': 1,\n",
       "  'from': 0,\n",
       "  'my': 0,\n",
       "  'regarding': 0,\n",
       "  'toward': 0,\n",
       "  'within': 0,\n",
       "  'be': 1,\n",
       "  'have': 1,\n",
       "  'near': 0,\n",
       "  'same': 0,\n",
       "  'towards': 0,\n",
       "  'without': 0,\n",
       "  'because': 0,\n",
       "  'he': 0,\n",
       "  'need': 0,\n",
       "  'several': 0,\n",
       "  'under': 0,\n",
       "  'worth': 0,\n",
       "  'before': 0,\n",
       "  'her': 0,\n",
       "  'neither': 0,\n",
       "  'she': 0,\n",
       "  'unless': 0,\n",
       "  'would': 0,\n",
       "  'behind': 0,\n",
       "  'him': 0,\n",
       "  'no': 2,\n",
       "  'should': 0,\n",
       "  'unlike': 0,\n",
       "  'yes': 0,\n",
       "  'below': 0,\n",
       "  'i': 0,\n",
       "  'nobody': 0,\n",
       "  'since': 0,\n",
       "  'until': 0,\n",
       "  'you': 0,\n",
       "  'beside': 0,\n",
       "  'if': 0,\n",
       "  'none': 0,\n",
       "  'so': 0,\n",
       "  'up': 0,\n",
       "  'your': 0},\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " {'alpha': 197, 'digit': 1, 'uppercase': 6, 'whitespace': 46, 'tab': 0},\n",
       " 0,\n",
       " {'he': 0, 'she': 0, 'they': 0}]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_freq_features_from_document(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_data(data):\n",
    "    flatten_data = []\n",
    "    for item in data:\n",
    "        if type(item) is dict:\n",
    "            for val in item.values():\n",
    "                flatten_data.append(val)\n",
    "        else:\n",
    "            flatten_data.append(val)\n",
    "    \n",
    "    return flatten_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_from_document(text):\n",
    "    return extract_num_features_from_document(text) + flatten_data(extract_freq_features_from_document(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(extract_features_from_document(text))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6ade73bf49f1256608149ff920ccd937fcccdc8efd4975ff38aa98fc4d821ac5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('fyp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
